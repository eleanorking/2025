
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data_cancer = load_breast_cancer()\n",
        "X_wis = data_cancer.data\n",
        "y_wis = data_cancer.target\n",
        "X_train_wis, X_test_wis, y_train_wis, y_test_wis = train_test_split(X_wis, y_wis, test_size=0.3, random_state=42)\n",
        "# np.random.seed(42)\n",
        "sc_2=StandardScaler()\n",
        "X_transform_wis =sc_2.fit_transform(X_train_wis)"
      ],
      "metadata": {
        "id": "8FzGm8ss_XOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Sigmoid or predict function (15 points)\n",
        "\\begin{align*}\n",
        "\\widehat{y}_i &= \\sigma_w(x_i) = \\frac{e^{w^Tx_i}}{1 + e^{w^Tx_i}} \\\\\n",
        "\\widehat{Y} &= \\begin{bmatrix} \\widehat{y}_1 \\\\\n",
        "\\widehat{y}_2 \\\\\n",
        "\\vdots \\\\\n",
        "\\widehat{y}_n\\end{bmatrix} \\\\\n",
        "X &= \\begin{bmatrix} x^T_1 \\\\\n",
        "x^T_2 \\\\\n",
        "\\vdots \\\\\n",
        "x^T_n \\end{bmatrix}\n",
        "\\end{align*}\n",
        "where $x_i$ is an instance of $X$ with dimensions $p \\times 1$\n",
        "Let's build a function that applies the sigmoid function to $X$ with dimensions $n \\times p$ with a given $w$ ($p \\times 1$) and it outputs $\\widehat{Y}$ with dimensions $n \\times 1$. Assume the intercept vector is already included in $X$\n",
        "\n",
        "Implement this without loops for full credit."
      ],
      "metadata": {
        "id": "5kyH4ZeNP9YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "# [Question 4 Part 1: 15 points]                                             #\n",
        "# TODO:                                                                     #\n",
        "# Implement the sigmoid function with the formula provided                  #\n",
        "# Input: X of shape (n, p), w of shape (p,)                                 #\n",
        "# Output: predicted y of shape (n, 1)                                       #\n",
        "#                                                                           #\n",
        "# ONLY use numpy for this section! Use of scikit-learn will give you 0 points\n",
        "#############################################################################\n",
        "\n",
        "def sigmoid(X, w):\n",
        "  #############################################################################\n",
        "  #                              START OF YOUR CODE                           #\n",
        "  #############################################################################\n",
        "   ## use a boolean matrix and convert it into an int array\n",
        "  y_hat = np.exp(np.dot(X, w)) / (1 + np.exp(np.dot(X,w)))\n",
        "  boolean_matrix = y_hat >= 0.5\n",
        "  y_hat_arr = boolean_matrix.astype(int)\n",
        "  return y_hat_arr\n",
        "\n",
        "  #############################################################################\n",
        "  #                              END OF YOUR CODE                             #\n",
        "  #############################################################################"
      ],
      "metadata": {
        "id": "OtlK7SWZQDGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test cases\n",
        "<font color=\"#de3023\"><h6><b>DO NOT MAKE EDITS TO THIS SECTION</b></h6></font>"
      ],
      "metadata": {
        "id": "h4vRrITWQOKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "exp_w = np.random.randn(X_transform_wis.shape[1],1)\n",
        "print(sigmoid(X_transform_wis, exp_w)[np.random.randint(X_transform_wis.shape[0], size = 10)])"
      ],
      "metadata": {
        "id": "4S9SxmraQPVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7478ddbc-d46e-479a-984b-e08b8e7d1b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Logisitic Regression (35 points)\n",
        "\n",
        "\n",
        "The loss function for Logistic Regression is:\n",
        "\\begin{align*}\n",
        "l_w(\\widehat{y}_i, y_i) &= \\left\\{\n",
        "  \\begin{array}{lr}\n",
        "        -\\log(\\sigma_w(x_i)), & \\text{if } y_i = 1\\\\\n",
        "        -\\log(1 - \\sigma_w(x_i)), & \\text{if } y_i = 0\n",
        "    \\end{array}\n",
        "  \\right\\} \\\\\n",
        "  &= -y_i\\log(\\sigma_w(x_i)) - (1 - y_i)\\log(1 - \\sigma_w(x_i))\\\\\n",
        "  L(w) &= \\frac{1}{n}\\sum_{i =1}^n l_w(\\widehat{y}_i, y_i)\n",
        "\\end{align*}\n",
        "\n",
        "The gradient of $L(w)$ is as follows:\n",
        "\\begin{align*}\\nabla_w L(w) &= \\frac{1}{n}\\sum_{i =1}^n \\left(\\widehat{y}_i - y_i\\right)x_i \\\\\n",
        "&= \\frac{1}{n} X^T\\left(\\widehat{Y} - Y\\right)\n",
        "\\end{align*}\n",
        "\n",
        "Write gradient descent for logistic regression which will output $\\widehat{w}$ and the training loss of the model using $\\widehat{w}$ with a given $X$ matrix with dimensions $n \\times p$, $Y$ vector with dimensions $n \\times 1$, $\\eta$ learning rate, $w_0$ initailization for $w$, and $\\epsilon$ convergence condition. This algorithm should also plot the losses across all iterations (similar to lab).\n",
        "\n",
        "Implement this using one loop for full credit.\n",
        "\n",
        "Remember, the psuedo-code for gradient descent is\n",
        "\n",
        "```\n",
        "\n",
        "function grad_descent(X,y,init_weights, eta, epsilon):\n",
        "  #suppose we had functions loss(y_hat,y) and Dloss(y_hat,y) that find\n",
        "  #the loss and the gradient of the loss along with function f(weights)\n",
        "  #that gives the function value\n",
        "\n",
        "  initalize grad_update to a 0 vector of length (p+1)\n",
        "  w = init_weights\n",
        "  while True:\n",
        "    for i in length(X):\n",
        "      w_old = w\n",
        "      y_hat = f(X[i], w)\n",
        "      grad_update += (1/n)*Dloss(y_hat,y, X[i])\n",
        "    w = w - eta*grad_update\n",
        "    if norm(w - w_old) < epsilon:\n",
        "      return w\n",
        "```"
      ],
      "metadata": {
        "id": "nF_QcGeIQaoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "# [Question 4 Part 2: 35 points]                                            #\n",
        "# TODO:                                                                     #\n",
        "# Implement Gradient Descent for Logistic Regression using the gradient.    #\n",
        "# formula from above.                                                       #\n",
        "# Input: X of shape (n, p), y of shape (n,), eta,                           #\n",
        "#        initial_w of shape ((p + 1), ), epsilon                            #\n",
        "# Output: w of shape ((p + 1), ) and Training loss using that weight.       #\n",
        "# Also plot your losses across all iterations                               #\n",
        "#                                                                           #\n",
        "# ONLY use numpy for this section! Use of scikit-learn will give you 0 points\n",
        "#############################################################################\n",
        "\n",
        "def log_grad_descent(X, y, eta, initial_w, epsilon):\n",
        "  n, p = X.shape\n",
        "  ones = np.ones((n, 1))\n",
        "  new_X = np.hstack((ones, X))\n",
        "  new_y = np.array(y).reshape((n, 1))\n",
        "  w = initial_w\n",
        "  losses_arr = []\n",
        "\n",
        "  while True:\n",
        "    logistic = np.dot(X, w)\n",
        "    y_pred = sigmoid(logistic)\n",
        "\n",
        "    loss = (1/n) * (np.sum(y*np.log(y_hat) + (1-y) * np.log(1-y_hat)))\n",
        "    losses_arr.append(loss)\n",
        "    d_loss = (1/n) * np.dot(X.T, (y_pred - y))\n",
        "\n",
        "    w -= eta * d_loss\n",
        "\n",
        "    if abs(losses_arr[0] - losses_arr[-1]) < epsilon:\n",
        "      break\n",
        "\n",
        "  plt.plot(range(1, len(losses_arr) + 1),w, label = 'loss')\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"iteration\")\n",
        "  plt.show()\n",
        "\n",
        "  return w, losses_arr\n",
        "\n",
        "\n",
        "\n",
        "  #############################################################################\n",
        "  #                              END OF YOUR CODE                             #\n",
        "  #############################################################################"
      ],
      "metadata": {
        "id": "baCrHaFmSCPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test cases\n",
        "<font color=\"#de3023\"><h6><b>DO NOT MAKE EDITS TO THIS SECTION</b></h6></font>"
      ],
      "metadata": {
        "id": "hHVo-w9vSJTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "new_p = X_transform_wis.shape[1] + 1\n",
        "w_graddescent , loss = log_grad_descent(X_transform_wis, y_train_wis, eta = 0.01, initial_w =np.random.randn(new_p,1), epsilon = 0.001)\n",
        "pred_y_test = sigmoid(np.hstack((np.ones((X_test_wis.shape[0], 1)), sc_2.transform(X_test_wis))) , w_graddescent)\n",
        "new_y = np.array(y_test_wis).reshape((y_test_wis.shape[0], 1))\n",
        "print(\"The training loss is {}\".format(loss))\n",
        "print(\"The test loss is {}\".format(-1/len(new_y) * (np.sum(np.multiply(np.log(pred_y_test), new_y)) + np.sum(np.multiply(np.log(1 - pred_y_test), 1-new_y)))))\n",
        "print(\"The weights of the first 10 variables are {}\".format(w_graddescent[1:10]))"
      ],
      "metadata": {
        "id": "J8nmx_9JSKHl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "39ed4fac-e972-45ae-e97e-17bbf028634d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-ed152a4f4b9c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnew_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_transform_wis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mw_graddescent\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_grad_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_transform_wis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_wis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpred_y_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_wis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_wis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mw_graddescent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_wis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_wis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-cc74aee56511>\u001b[0m in \u001b[0;36mlog_grad_descent\u001b[0;34m(X, y, eta, initial_w, epsilon)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mlogistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (398,30) and (31,1) not aligned: 30 (dim 1) != 31 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Logistic Regression with multiple labels (50 points)\n",
        "Now, let's try to extend this idea to logistic regression with multiple labels"
      ],
      "metadata": {
        "id": "SWZ1hZJsSzq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data_iris = load_iris()\n",
        "X_iris = data_iris.data\n",
        "y_iris = data_iris.target\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "# np.random.seed(42)\n",
        "sc_3=StandardScaler()\n",
        "X_transform_iris =sc_3.fit_transform(X_train_iris)"
      ],
      "metadata": {
        "id": "G4QCUKep_7wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - One hot encoding (15 points)\n",
        "\n",
        "If we have a vector $z$ with dimensions $n \\times 1$ that contains $m$ labels stored in $l$ with dimensions $m \\times 1$, the one hot encoding algorithm should return an $n \\times m$ matrix $Y$ where $Y_{ij}$ is 1 if $z_i = l_j$ else it is 0.\n",
        "\\begin{align*}\n",
        "Y = \\begin{bmatrix} y_1^T \\\\\n",
        "y_2^T \\\\\n",
        "\\vdots \\\\\n",
        "y_n^T \\end{bmatrix} = \\begin{bmatrix} y_{11} & \\cdots & y_{1m} \\\\\n",
        "y_{21} & \\cdots & y_{2m}\\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "y_{n1} & \\cdots & y_{nm} \\end{bmatrix}\n",
        "\\end{align*}\n",
        "where $y_i$ is $m \\times 1$ vector and it contains the encodings of $z_i$\n",
        "\n",
        "\n",
        "For example,\n",
        "If\n",
        "\\begin{align*}\n",
        "z = \\begin{bmatrix} Sahana \\\\\n",
        "Eduardo \\\\\n",
        "Jake \\\\\n",
        "Eduardo \\\\\n",
        "Jake \\\\\n",
        "Jake \\\\\n",
        "Eduardo \\end{bmatrix}, l = \\begin{bmatrix} Sahana \\\\\n",
        "Eduardo \\\\\n",
        "Jake  \\end{bmatrix}\n",
        "\\end{align*}\n",
        "Then $Y$ would be\n",
        "\\begin{align*}\n",
        "Y = \\begin{bmatrix} 1 & 0 & 0 \\\\\n",
        "0 & 1& 0\\\\\n",
        "0 & 0 & 1\\\\\n",
        "0 & 1& 0\\\\\n",
        "0 & 0 & 1\\\\\n",
        "0 & 0 & 1\\\\\n",
        "0 & 1 & 0\\\\  \\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "You can think of the 1st column checking whether $z_i$ is Sahana, 2nd column checking whether $z_i$ is Eduardo, and 3rd column checking whether $z_i$ is Jake.\n",
        "\n",
        "Implement this one hot encoding algorithm that returns $Y$ ($n \\times m$) given $z$ ($n \\times 1$) and labels $l$ ($m \\times 1$)\n",
        "\n",
        "Implement this without loops for full credit.\n",
        "\n",
        "**Note: In each row, only one entry can be 1**\n"
      ],
      "metadata": {
        "id": "MpaJPkLSSzzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "# [Question 5 Part 1: 15 points]                                             #\n",
        "# TODO:                                                                     #\n",
        "# Implement one hot encoding with a given y and l                           #\n",
        "# Input: z of shape (n, 1), l of shape (m,1)                                #\n",
        "# Output: Y of shape (n,m)                                                  #\n",
        "#                                                                           #\n",
        "#############################################################################\n",
        "\n",
        "def one_hot_encoding(z, l):\n",
        "  #############################################################################\n",
        "  #                              START OF YOUR CODE                           #\n",
        "  #############################################################################\n",
        "  # Replace \"pass\" statement with your code\n",
        "  Y = []\n",
        "  Y = (z == l.T)\n",
        "\n",
        "  return Y.astype(int)\n",
        "  #############################################################################\n",
        "  #                              END OF YOUR CODE                             #\n",
        "  #############################################################################"
      ],
      "metadata": {
        "id": "9Yox5qSgS9BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test cases\n",
        "<font color=\"#de3023\"><h6><b>DO NOT MAKE EDITS TO THIS SECTION</b></h6></font>"
      ],
      "metadata": {
        "id": "fCiionCuT0S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "l = np.arange(3)\n",
        "print(one_hot_encoding(y_iris.reshape((y_iris.shape[0], 1)), l)[np.random.randint(150, size = 10)])"
      ],
      "metadata": {
        "id": "3Jx863ONT1UF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd0eecf3-98c6-4733-f181-bf5895daabe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Coding Implementation of Gradient Descent (35 points)\n",
        "\n",
        "The loss function and gradient of it for this is:\n",
        "\\begin{align*}\n",
        "l_{W}(\\widehat{y}_i, y_i) &= -\\sum_{j = 1}^{m}y_{ij}\\log(\\sigma_{w_j}(x_i))\\\\\n",
        "  L(W) &= \\frac{1}{n}\\sum_{i =1}^n l_{W}(\\widehat{y}_i, y_i) \\\\\n",
        "  \\frac{\\partial l_{W}(\\widehat{y}_i, y_i)}{\\partial w_j} &= x_i(\\widehat{y}_{ij} - y_{ij})\\\\\n",
        "  \\frac{\\partial L(W)}{\\partial w_j} &= \\frac{1}{n}\\sum_{i=1}^{n}x_i(\\widehat{y}_{ij} - y_{ij})\n",
        "\\end{align*}\n",
        "\n",
        "Using this information, implement gradient descent algorithm which will output $\\widehat{W}$ and the training loss of the model using $\\widehat{W}$ with a given $X$ matrix with dimensions $n \\times p$, $y$ vector with dimensions $n \\times 1$, $\\eta$ learning rate, $W_0$ initailization for $W$, and $\\epsilon$ convergence condition. **This algorithm should also plot the losses across all iterations** (similar to lab).\n",
        "\n",
        "Implement this in two loops for full credit\n",
        "\n",
        "The psuedocode in question 1 part 2 can also be applicable here\n",
        "\n",
        "\n",
        "\n",
        "**Extra Credit (5 points)**: Implement this algorithm using only one loop\n",
        "\n",
        "**Hint:** There are two approaches (that I can think of) -\n",
        "\n",
        "1.   You can update each $w_j$ in $W$ using the derivative above (easier to do)\n",
        "2.   Using the derivative above, Find $\\nabla_W L(W)$ in matrix form and use this derivative to update $W$ all at once (this ties into the extra credit and is similar to how the psuedocode is set up)"
      ],
      "metadata": {
        "id": "-43a6KJgT-d0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a function that outputs $$\\sigma_{w_j}(w)$$"
      ],
      "metadata": {
        "id": "FwaEeiLFCCcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(X, W):\n",
        "  arb = np.exp(X @ W.T)\n",
        "  sums = np.tile(np.sum(arb, axis = 1).reshape((arb.shape[0], 1)), (1, arb.shape[1]))\n",
        "\n",
        "  return np.divide(arb, sums)"
      ],
      "metadata": {
        "id": "AQgoqyR6CE8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################################\n",
        "# [Question 5 Part 2: 35 points]                                            #\n",
        "# TODO:                                                                     #\n",
        "# Implement Gradient Descent for Logistic Regression using the gradient.    #\n",
        "# formula from above.                                                       #\n",
        "# Input: X of shape (n, p), y of shape (n,), eta,                           #\n",
        "#        initial_W of shape (m, (p + 1)), epsilon                           #\n",
        "# Output: W of shape (m, (p + 1)) and Training loss using that weight.      #\n",
        "# Also plot your losses across all iterations                               #\n",
        "#                                                                           #\n",
        "# ONLY use numpy for this section! Use of scikit-learn will give you 0 points\n",
        "#############################################################################\n",
        "\n",
        "def multilog_grad_descent(X, y, eta, initial_W, epsilon):\n",
        "  #############################################################################\n",
        "  #                              START OF YOUR CODE                           #\n",
        "  #############################################################################\n",
        "  n, p = X.shape\n",
        "  ones = np.ones((n, 1))\n",
        "  new_X = np.hstack((ones, X))\n",
        "  new_y = np.array(y).reshape((n, 1))\n",
        "  # Replace \"pass\" statement with your code\n",
        "\n",
        "  W = initial_W\n",
        "  prev_loss = 0\n",
        "\n",
        "  while True:\n",
        "    y_hat = X.dot(W)\n",
        "    d_loss = (1/n) * np.sum(X * (y_hat - y), axis = 0)\n",
        "\n",
        "    W = W - eta * d_loss\n",
        "\n",
        "    new_loss = (1/n) * np.sum(X * (y_hat - y) ** 2)\n",
        "\n",
        "    if abs(prev_loss - new_loss) < epsilon:\n",
        "      break\n",
        "\n",
        "    prev_loss = new_loss\n",
        "\n",
        "    soft_max()\n",
        "\n",
        "    plt.plot(range(1, len() + 1),w, label = 'loss')\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"iteration\")\n",
        "    plt.show()\n",
        "\n",
        "    return W, new_loss\n",
        "\n",
        "  #############################################################################\n",
        "  #                              END OF YOUR CODE                             #\n",
        "  #############################################################################"
      ],
      "metadata": {
        "id": "biApDn1UULuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test cases\n",
        "<font color=\"#de3023\"><h6><b>DO NOT MAKE EDITS TO THIS SECTION</b></h6></font>"
      ],
      "metadata": {
        "id": "UHc-v6eBCK6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "new_p = X_transform_iris.shape[1] + 1\n",
        "W_graddescent, training_loss = multilog_grad_descent(X_transform_iris, y_train_iris, eta = 0.01, initial_W =np.random.randn(3, new_p), epsilon = 0.001)\n",
        "pred_y_test = softmax(np.hstack((np.ones((X_test_iris.shape[0], 1)), sc_3.transform(X_test_iris))) , W_graddescent)\n",
        "new_Y_test = one_hot_encoding(y_test_iris.reshape((y_test_iris.shape[0], 1)) , np.arange(3))\n",
        "print(\"The training loss is {}\".format(training_loss))\n",
        "print(\"The test loss is {}\".format(-1/len(pred_y_test) * np.sum(np.multiply(np.log(pred_y_test), new_Y_test))))\n",
        "print(\"The weights are \\n {}\".format(W_graddescent))"
      ],
      "metadata": {
        "id": "pkcxMTpZCLE0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "7ea079f8-656c-478d-e473-ef68dba3705b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-1ac98349644a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnew_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_transform_iris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mW_graddescent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultilog_grad_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_transform_iris\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_iris\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_W\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpred_y_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_iris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_iris\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mW_graddescent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_Y_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_iris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_iris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-bb0b24251153>\u001b[0m in \u001b[0;36mmultilog_grad_descent\u001b[0;34m(X, y, eta, initial_W, epsilon)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (105,4) and (3,5) not aligned: 4 (dim 1) != 3 (dim 0)"
          ]
        }
      ]
    }
  ]
}
